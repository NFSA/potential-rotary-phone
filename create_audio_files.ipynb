{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "secrets_file = Path(\".\") / \"secrets.json\"\n",
    "with open(secrets_file) as f:\n",
    "    secrets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import gc \n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "audio_file = \"sample/two-mates-having-a-chat.m4a\"\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" if device == \"cuda\" else \"int8\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "\n",
    "# 1. Transcribe with original whisper (batched)\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
    "\n",
    "# save model to local path (optional)\n",
    "# model_dir = \"/path/\"\n",
    "# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n",
    "\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "print(result[\"segments\"]) # before alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
    "\n",
    "# 2. Align whisper output\n",
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "\n",
    "print(result[\"segments\"]) # after alignment\n",
    "\n",
    "# delete model if low on GPU resources\n",
    "# import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
    "\n",
    "# 3. Assign speaker labels\n",
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=secrets.get(\"hf_token\"), device=device)\n",
    "# add min/max number of speakers if known\n",
    "diarize_segments = diarize_model(audio)\n",
    "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "result = whisperx.assign_word_speakers(diarize_segments, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment in result[\"segments\"]:\n",
    "    print(segment[\"speaker\"], segment[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "\n",
    "# Create a directory to save the audio files\n",
    "output_dir = Path(\"output_audio\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Iterate through the segments and create audio files for each speaker\n",
    "for i, segment in enumerate(result[\"segments\"]):\n",
    "    start_time = segment[\"start\"]\n",
    "    end_time = segment[\"end\"]\n",
    "    speaker = segment[\"speaker\"].replace(\"_\", \"\")\n",
    "    output_file = output_dir / f\"{speaker}_{i}.wav\"\n",
    "    \n",
    "    # Skip if output_file exists\n",
    "    if not output_file.exists():\n",
    "        # Extract the audio segment for the speaker\n",
    "        (\n",
    "            ffmpeg\n",
    "            .input(audio_file, ss=start_time, to=end_time)\n",
    "            .output(str(output_file), format='wav', loglevel=\"quiet\")\n",
    "            .run(overwrite_output=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ffmpeg\n",
    "\n",
    "# Create lists of files for each speaker\n",
    "speaker_files = {}\n",
    "for file in output_dir.glob(\"*.wav\"):\n",
    "    speaker, index = file.stem.split('_')\n",
    "    if speaker not in speaker_files:\n",
    "        speaker_files[speaker] = []\n",
    "    speaker_files[speaker].append((int(index), file))\n",
    "\n",
    "# Sort the files for each speaker by the index\n",
    "for speaker in speaker_files:\n",
    "    speaker_files[speaker].sort()\n",
    "\n",
    "# Merge the files for each speaker into a single audio file\n",
    "for speaker, files in speaker_files.items():\n",
    "    # Use the first file as the initial input to concatenate with\n",
    "    concat = ffmpeg.input(str(files[0][1]))\n",
    "    for _, file in files[1:]:\n",
    "        segment = ffmpeg.input(str(file))\n",
    "        concat = ffmpeg.concat(concat, segment, v=0, a=1)\n",
    "    output_file = output_dir / f\"{speaker}_merged.wav\"\n",
    "    try:\n",
    "        concat.output(str(output_file), format='wav', loglevel=\"quiet\").run(overwrite_output=True)\n",
    "        for _, file in files:\n",
    "            file.unlink()\n",
    "    except ffmpeg.Error as e:\n",
    "        print(f\"Error occurred: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "potential-rotary-phone-7QSl72Ex-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
